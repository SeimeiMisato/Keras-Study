{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# RNN から Attention までのモデルを定義できるようになる"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 深層学習での前処理の順序\n",
    "\n",
    "分かち書き・正規化\n",
    "- 単語毎に分割(分かち書き)・正規化(無駄な文字を削除)\n",
    "\n",
    "ここまでをクレンジング(cleansing)という。\n",
    "\n",
    "↓\n",
    "\n",
    "単語埋め込み(Word Embedding)\n",
    "- 単語をベクトル化\n",
    "\n",
    "または、\n",
    "\n",
    "単語をトークン化(Tokenize)\n",
    "- 単語を一意の ID に変換\n",
    "\n",
    "↓\n",
    "\n",
    "パディング(Padding)・端数処理(truncating)\n",
    "\n",
    "↓\n",
    "\n",
    "学習モデルに入力"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Word Embedding(単語埋め込み)\n",
    "\n",
    "古典的なNLPでは単語を数値に変換して計算を行う。\n",
    "\n",
    "- One hot vector\n",
    "- TFIDF\n",
    "\n",
    "しかし、これらの手法は、「**スパース**(まばらで単語の本質的な特徴が少ない)」なので、効率が悪い。\n",
    "一方、ニューラルネットワークでは、「**Embedding**(各単語にわずか数百次元のベクトルを割り当てる)」を行うことで、「**Word2Vec**」 のように似た意味を持った単語ベクトルを得られる。\n",
    "\n",
    "Word Embedding のことを、「**分散表現**」とも言う。\n",
    "\n",
    "---\n",
    "\n",
    "`tensorflow.keras`では、`Embedding Layer`で Embedding を行う。\n",
    "\n",
    "Embedding への入力は、各単語に割り当てた ID からなる行列(**Embedding Matrix**)となる。\n",
    "\n",
    "これら３つで、最低限指定できる。\n",
    "\n",
    "- input_dim: 語彙数、単語の種類の数\n",
    "- output_dim: 単語ベクトルの次元\n",
    "- input_length: 各文の長さ\n",
    "\n",
    "以下を追加↓\n",
    "\n",
    "`from tensorflow.keras.layers import Embedding`\n",
    "\n",
    "`vocab_size = 1000 # 扱う語彙の数`\n",
    "\n",
    "`embedding_dim = 100 # 単語ベクトルの次元`\n",
    "\n",
    "`seq_length = 20 # 文の長さ`\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "また、頻度が一定以下の単語は学習にあまり貢献しないので、 Unknown(未知語)にすると、メモリを節約できる。`<unk>`で表現する。\n",
    "\n",
    "おもしろページ↓\n",
    "\n",
    "[単語埋め込み (Word embeddings):Tensorflow](https://www.tensorflow.org/tutorials/text/word_embeddings?hl=ja)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "batch_size = 32 # バッチサイズ(一度に並列計算を行うデータ数)\n",
    "\n",
    "input_data = np.arange(batch_size * seq_length).reshape(batch_size, seq_length)\n",
    "\n",
    "model = Sequential()\n",
    "# Embeddingを追加 -> keras embedding で検索\n",
    "\n",
    "output = model.predict(input_data)\n",
    "\n",
    "# Embedding Matrix の要素を得られる\n",
    "print(output.shape)"
   ]
  },
  {
   "source": [
    "## 活性化関数(Activation function)\n",
    "\n",
    "ニューラルネットワークでは、非線形関数で出力することで複雑なモデル(非線形モデル)を近似(**学習**)できる\n",
    "\n",
    "入力 $=$ 入力 $\\times$ 重み $+$ バイアス\n",
    "\n",
    "出力 $=$ **活性化関数** $($ 複数のニューロンからの入力 $\\times$ 各重み $+$ バイアス $)$\n",
    "\n",
    "[ニューラルネットワークの基礎](https://tutorials.chainer.org/ja/13_Basics_of_Neural_Networks.html)\n",
    "\n",
    "---\n",
    "\n",
    "tensorflow.keras.models.Sequentialを使わずにこのようにモデルを記述する方法を`Functional API`という。\n",
    "\n",
    "`y = Activation('softmax')(x)`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "x = Input(shape=(20, 5))\n",
    "# xにsoftmaxを作用させる\n",
    "\n",
    "\n",
    "model = Model(inputs=x, outputs=y)\n",
    "\n",
    "sample_input = np.ones((12, 20, 5))\n",
    "sample_output = model.predict(sample_input)\n",
    "\n",
    "print(np.sum(sample_output, axis=2))"
   ]
  },
  {
   "source": [
    "## RNN(Recurrent Neural Network, 再帰ニューラルネットワーク)\n",
    "\n",
    "「**可変長系列**(任意の長さの入力)」の時系列データの分析に使える。\n",
    "\n",
    "改良された亜種が多く、Simple RNN が使われることは少ないが、\n",
    "\n",
    "`from tensorflow.keras.layers import SimpleRNN` でモジュールをインポートし、\n",
    "\n",
    "`model.add(SimpleRNN(ユニット数, return_sequences=True))`\n",
    "\n",
    "でモデルを追加できる。\n",
    "\n",
    "---\n",
    "\n",
    "- 時間：$t$\n",
    "- 活性化関数：$f$\n",
    "- 隠れ状態ベクトル(隠れ層のベクトル)：$h_t$\n",
    "- 時刻$t$の入力：$y_t$\n",
    "- 時刻$t$の出力：$y_t$\n",
    "- 学習可能な行列：$W_h, W_y, U$\n",
    "- バイアス：$b_h, b_y$\n",
    "\n",
    "$\\color{red}{Uh_{t-1}}$ で隠れ層の時間を戻している。$h_{t-1}$でデータをループさせ、時系列を分析できる。\n",
    "\n",
    "$\\begin{align*}\n",
    "h_t &= f^{\\rm{(hidden})}(W_{h}x_t + \\color{red}{Uh_{t-1}} + b_{h})\\\\\n",
    "y_{t} &= f^{(\\rm{out})}(W_{y}h_t + b_{y}) \n",
    "\\end{align*}$\n",
    "\n",
    "分かりやすい図↓\n",
    "\n",
    "https://axa.biopapyrus.jp/deep-learning/rnn/\n",
    "\n",
    "1990年、エルマンネット(RNNの原形)発表。\n",
    "\n",
    "https://www.cis.twcu.ac.jp/~asakawa/waseda2002/elman.pdf\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 過学習(Over fitting)防止の戦略\n",
    "\n",
    "ニューラルネットワークでは、学習を行い過ぎることで訓練データ(Train Data)では、十分な精度を出せるが、未知のデータに対して全く精度を出せない場合がある。\n",
    "\n",
    "### 汎化性能\n",
    "\n",
    "未知のデータでも高い性能を出せる性能を「**汎化性能**」という。\n",
    "\n",
    "ニューラルネットワークの学習では、学習しすぎ(**過学習**)でも、学習不足でもいけない。\n",
    "\n",
    "\n",
    "### 局所最適解と大域最適解(全体最適解)\n",
    "\n",
    "過学習は、与えられたデータ内だけで、局所最適解に陥っている状態。\n",
    "\n",
    "ニューラルネットワークでは、大域最適解に近づけることが目的。\n",
    "\n",
    "[全体最適解と局所最適解](https://qiita.com/9ryuuuuu/items/ab361d00af32f71c1f89)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Dropout(ドロップアウト)\n",
    "\n",
    "訓練時、ランダム(今回、`0.3`)にニューロン(ユニット)を`0`(不活性化)にして、過学習を防ぐ手法。\n",
    "\n",
    "---\n",
    "\n",
    "`y = Dropout(0.3)(x)`\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "batch_size = 32  # バッチサイズ\n",
    "vocab_size = 1000  # 扱う語彙の数\n",
    "embedding_dim = 100  # 単語ベクトルの次元\n",
    "seq_length = 20  # 文1の長さ\n",
    "lstm_units = 200  # LSTMの隠れ状態ベクトルの次元数\n",
    "\n",
    "input = Input(shape=(seq_length,))\n",
    "\n",
    "embed = Embedding(input_dim=vocab_size, output_dim=embedding_dim,\n",
    "                  input_length=seq_length)(input)\n",
    "\n",
    "rnn = SimpleRNN(lstm_units, return_sequences=True)(embed)\n",
    "\n",
    "output = \n",
    "\n",
    "model = Model(inputs=input, outputs=output)\n",
    "\n",
    "sample_input = np.arange(\n",
    "    batch_size * seq_length).reshape(batch_size, seq_length)\n",
    "\n",
    "sample_output = model.predict(sample_input)\n",
    "\n",
    "print(sample_output.shape)"
   ]
  },
  {
   "source": [
    "### 正則化\n",
    "\n",
    "L1正規化\n",
    " - 重み係数の絶対値に比例するコストを加える\n",
    "\n",
    "L2正則化\n",
    " - 重み係数の二乗に比例するコストを加える。\n",
    "\n",
    "がある。\n",
    "\n",
    "レイヤーの重みに制約を付けることでパラメータを単純にすることで、モデルも単純になる。\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## LSTM(Long Short Term Memory, 長短期記憶)\n",
    "\n",
    "RNNの改良版で、RNNに時間情報を出し入れする複数のゲートを設けることで長期記憶と短期記憶を実現している。\n",
    "\n",
    "1997年発表。\n",
    "\n",
    "https://www.bioinf.jku.at/publications/older/2604.pdf\n",
    "\n",
    "---\n",
    "\n",
    "以下を追加↓\n",
    "\n",
    "`from tensorflow.keras.layers import LSTM`\n",
    "\n",
    "`lstm_units = 200` # LSTMの隠れ状態ベクトルの次元数(100~300程度が目安)\n",
    "\n",
    "そして、モデルに`LSTM`を追加する。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "# モジュールをインポート\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 32 # バッチサイズ(一度に並列計算を行うデータ数)\n",
    "vocab_size = 1000 # 扱う語彙の数\n",
    "embedding_dim = 100 # 単語ベクトルの次元\n",
    "seq_length = 20 # 文の長さ\n",
    "\n",
    "input_data = np.arange(batch_size * seq_length).reshape(batch_size, seq_length)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=seq_length))\n",
    "# LSTMを追加 -> keras lstm で検索\n",
    "\n",
    "\n",
    "\n",
    "output = model.predict(input_data)\n",
    "print(output.shape)"
   ]
  },
  {
   "source": [
    "## BiLSTM(Bi-Directional Recurrent Neural Network, 双方向性再帰型ニューラルネットワーク)\n",
    "\n",
    "従来のLSTMと同じ過去の情報に加え、予測した未来の情報も加える。過去から未来までのすべての入力系列を加味して分析することで、高い精度を得られる手法。\n",
    "\n",
    "未来への順伝播 $x={x1, ... ,xT}$ \n",
    "\n",
    "と\n",
    "\n",
    "過去への逆伝播 $x={xT, ... , x1}$ の双方向に伝播するネットワーク。\n",
    "\n",
    "図↓\n",
    "\n",
    "https://axa.biopapyrus.jp/deep-learning/rnn/brnn.html\n",
    "\n",
    "1997年。Bidirectional recurrent neural net-works. として、発表。\n",
    "\n",
    "---\n",
    "\n",
    "モジュールをインポート\n",
    "\n",
    "`from tensorflow.keras.layers import Bidirectional`\n",
    "\n",
    "\n",
    "`tensorflow.keras`では、双方向の入力系列を繋げるためのオプションは複数ある。\n",
    "\n",
    "- sum: 要素和\n",
    "- mul: 要素積\n",
    "- concat: 結合\n",
    "- ave: 平均\n",
    "- None: 結合せずにlistを返す\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "\n",
    "batch_size = 32 # バッチサイズ\n",
    "vocab_size = 1000 # 扱う語彙の数\n",
    "embedding_dim = 100 # 単語ベクトルの次元\n",
    "seq_length = 20 # 文の長さ\n",
    "lstm_units = 200 # LSTMの隠れ状態ベクトルの次元数\n",
    "\n",
    "input_data = np.arange(batch_size * seq_length).reshape(batch_size, seq_length)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=seq_length))\n",
    "\n",
    "# LSTMをの入力系列を要素和させる -> keras bidirectional で検索\n",
    "\n",
    "\n",
    "# 次元数が減っている事が分かる\n",
    "output = model.predict(input_data)\n",
    "print(output.shape)"
   ]
  },
  {
   "source": [
    "## Attention\n",
    "\n",
    "深層学習における初の実用的な機械翻訳モデル。\n",
    "\n",
    "$t$の各時刻において$s$の各時刻の隠れ状態ベクトルを考慮した特徴を計算する。\n",
    "\n",
    "加重平均\n",
    "\n",
    "**双方向でも適用可能**なので**BERT**や**GPT-2**といったAttentionを応用したモデルが台頭する。\n",
    "\n",
    "[Neural Machine Translation by Jointly Learning to Align and Translate(Bahdanau 2015)](https://arxiv.org/abs/1409.0473)で発表。\n",
    "\n",
    "[Attention Is All You Need(Vaswani et al, 2017)](https://arxiv.org/abs/1706.03762) で Attention で構成された **Transformer** が開発され、汎用的に高い精度を叩き出し、有名になった。\n",
    "\n",
    "---\n",
    "\n",
    "`dot([u, v], axes=2)`は、`u`と`v`のバッチごとの行列積を計算します。\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import dot, concatenate\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "batch_size = 32 # バッチサイズ\n",
    "vocab_size = 1000 # 扱う語彙の数\n",
    "embedding_dim = 100 # 単語ベクトルの次元\n",
    "seq_length1 = 20 # 文1の長さ\n",
    "seq_length2 = 30 # 文2の長さ\n",
    "lstm_units = 200 # LSTMの隠れ状態ベクトルの次元数\n",
    "hidden_dim = 200 # 最終出力のベクトルの次元数\n",
    "\n",
    "# 2つのLSTMに共通のEmbeddingLayerを使うため、はじめにEmbeddingLayerを定義します。\n",
    "embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "\n",
    "input1 = Input(shape=(seq_length1,))\n",
    "embed1 = embedding(input1)\n",
    "bilstm1 = Bidirectional(LSTM(lstm_units, return_sequences=True), merge_mode='concat')(embed1)\n",
    "\n",
    "input2 = Input(shape=(seq_length2,))\n",
    "embed2 = embedding(input2)\n",
    "bilstm2 = Bidirectional(LSTM(lstm_units, return_sequences=True), merge_mode='concat')(embed2)\n",
    "\n",
    "# 要素ごとの積を計算する\n",
    "product = dot([bilstm2, bilstm1], axes=2) # サイズ：[バッチサイズ、文2の長さ、文1の長さ]\n",
    "\n",
    "# ここにAttention mechanismを実装してください\n",
    "a = \n",
    "c = \n",
    "c_bilstm2 = \n",
    "h = \n",
    "\n",
    "model = Model(inputs=[, ], outputs=)\n",
    "\n",
    "sample_input1 = np.arange(batch_size * seq_length1).reshape(batch_size, seq_length1)\n",
    "sample_input2 = np.arange(batch_size * seq_length2).reshape(batch_size, seq_length2)\n",
    "\n",
    "sample_output = model.predict([sample_input1, sample_input2])\n",
    "print(sample_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}